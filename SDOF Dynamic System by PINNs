Solves a forced SDOF dynamic system by training a Physics-Informed Neural Network to satisfy the governing equation and initial conditions.
Validates the PINN displacement response against an ODEINT reference solution and reports accuracy through error and loss plots.

Code: 
# ============================================================
#  SDOF Dynamic System - PINN Forward Solver
# ============================================================
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.autograd import grad
from scipy.integrate import odeint

# ------------------------------------------------------------
# 1. System Parameters
# ------------------------------------------------------------
m = 202472.0
c = 0.2
k = 47520000.0

# Provided time history data with dt = 0.02
force_data = np.array([

-0.0110, -0.0103, -0.00897, -0.00969, -0.0122, -0.0145, -0.0131, -0.0112, -0.00867, -0.00867, -0.0134, -0.0179, -0.0198, -0.0165, -0.0147, -0.0110, -0.00836, -0.00428, -0.00673, -0.0134, -0.0194, -0.0200, -0.00673, 0.00306, 0.0144, -0.00500, -0.0131, -0.0147, -0.0207, -0.0265, -0.0331, -0.0312, -0.0175, -0.0201, -0.0166, -0.0167, -0.00683, 0.00255, 0.0153, 0.0241, 0.0257, 0.0343, 0.0472, 0.0502, 0.0427, 0.0366, 0.0276, 0.0240, 0.0346, 0.0420, 0.0540, 0.0652, 0.0746, 0.0665, 0.0611, 0.0408, 0.0408, 0.00642, -0.0525, -0.0803, -0.0615, -0.0494, -0.0255, -0.00602, 0.0137, 0.0314, 0.0509, 0.0724, 0.101, 0.124, 0.156, 0.148, 0.118, 0.0953, 0.0910, 0.0944, 0.0856, 0.0919, 0.101, 0.123, 0.0334, -0.151, -0.211, -0.203, -0.207, -0.186, -0.176, -0.178, -0.178, -0.185, -0.166, -0.138, -0.111, -0.0797, -0.0437, -0.00173, 0.0367, 0.0800, 0.118, 0.163, 0.200, 0.246, 0.278, 0.310, 0.326, 0.349, 0.288, 0.237, -0.122, -0.242, -0.167, -0.191, -0.112, -0.0768, -0.0176, 0.0115, 0.0544, 0.0913, 0.121, 0.179, 0.0587, -0.268, -0.158, -0.176, -0.103, -0.0590, 0.0242, -0.0683, -0.202, -0.167, -0.172, -0.151, -0.125, -0.102, -0.0766, -0.0533, -0.0276, -0.00449, 0.0192, -0.00969, -0.0442, -0.0855, -0.0970, -0.0730, -0.0611, -0.0341, -0.0110, 0.0189, 0.0428, 0.0686, -0.00989, -0.0379, -0.00408, 0.00112, 0.0351, 0.0576, 0.0900, 0.115, 0.139, 0.0223, 0.0246, 0.0696, 0.0703, 0.135, 0.138, 0.208, -0.0949, -0.134, -0.0706, -0.0557, 0.00734, 0.0688, -0.109, -0.152, -0.109, -0.118, -0.0777, -0.0570, -0.0219, -0.0128, -0.0687, -0.0330, -0.0344, -0.0111, 0.00173, 0.0305, 0.0498, 0.0620, 0.0226, -0.00326, -0.0250, 0.00785, 0.0215, 0.0579, 0.0842, 0.123, 0.151, 0.177, 0.0429, 0.00296, 0.0264, 0.0299, -0.00561, -0.0150, 0.0146, 0.0210, 0.0509, 0.0658, 0.0976, 0.115, 0.148, 0.166, 0.199, 0.190, 0.202, 0.180, 0.127, -0.123, -0.0553, -0.0392, -0.0317, -0.114, -0.169, -0.251, -0.207, -0.188, -0.135, -0.0979, -0.0331, 0.0157, 0.0832, 0.135, 0.186, -0.00591, -0.0172, 0.0291, 0.0456, 0.100, 0.145, 0.189, 0.251, 0.172, -0.141, -0.102, -0.111, -0.0925, -0.0478, -0.127, -0.215, -0.165, -0.172, -0.134, -0.113, -0.0788, -0.0520, -0.0555, -0.122, -0.123, -0.118, -0.117, -0.0731, -0.0557, 0.00653, -0.0820, -0.166, -0.0876, -0.0980, -0.0404, -0.0150, 0.0325, 0.0661, 0.0893, 0.0481, 0.0202, -0.00275, 0.0298, 0.0454, 0.0800, 0.105, 0.138, 0.164, 0.190, 0.131, 0.0653, 0.0208, 0.0320, 0.0380, 0.0506, 0.0240, -0.00857, -0.0171, -0.0115, -0.0234, -0.0253, -0.0160, -0.00704, 0.0150, 0.0386, 0.0590, 0.0260, -0.00418, -0.0436, -0.0136, 0.00969, 0.0235, -0.0132, -0.00510, 0.00816, 0.0214, 0.0387, 0.0520, 0.0160, -0.00326, -0.0113, 0.000510, 0.00775, 0.00357, -0.00969, -0.00367, -0.00163, 0.00387, 0.00867, -0.00571, -0.0310, -0.0429, -0.0249, -0.0241, -0.0180, -0.0132, -0.00184, 0.0207, -0.0110, -0.00928, -0.00347, -0.0108, -0.0113, -0.0101, -0.000204, 0.00744, 0.0240, 0.0362, 0.0719, 0.0794, 0.0188, -0.0268, -0.0126, -0.00428, 0.0162, 0.00489, -0.0223, -0.0476, -0.0436, -0.0220, -0.00438, 0.0162, 0.0326, 0.0427, 0.0125, -0.0163, -0.0208, -0.00836, -0.0210, -0.0140, -0.00561, 0.00540, 0.0137, 0.0271, 0.0237, 0.00806, -0.000816, 0.0204, 0.0444, 0.0502, 0.0195, 0.00938, -0.00224, -0.00214, 0.00530, 0.00948, 0.0260, 0.0375, 0.0535, 0.0552, 0.0433, 0.0406, 0.0570, 0.0771, 0.0372, 0.0419, 0.00999, -0.0208, -0.0254, -0.0413, -0.0421, -0.0480, -0.0442, -0.0467, -0.00581, 0.0182, -0.0212, -0.0502, -0.0540, -0.0369, -0.0413, -0.0314, -0.0322, -0.0270, -0.0270, -0.0274, -0.0352, -0.0315, -0.0221, -0.00795, 0.00887, 0.0287, 0.0316, 0.0365, 0.0348, 0.0365, 0.0293, 0.0311, 0.0114, 0.0218, 0.0139, 0.0392, -0.0878, -0.138, -0.137, -0.138, -0.121, -0.106, -0.0845, -0.0664, -0.0453, -0.0263, -0.00612, -0.00928, -0.0186, -0.0150, 0.00867, 0.0166, 0.00510, 0.0269, 0.0593, 0.0884, 0.122, 0.173, 0.113, -0.112, -0.0373, -0.0454, -0.0241, -0.0979, -0.0669, -0.0609, -0.0683, -0.0563, -0.00275, 0.0385, 0.109, 0.170, 0.0966, 0.0416, 0.0680, 0.0135, -0.00969, -0.0530, -0.0843, -0.117, -0.117, -0.0819, -0.0376, 0.00296, 0.0556, 0.120, 0.164, -0.0275, 0.00347, -0.00571, 0.00204, 0.0149, 0.0548, 0.0814, -0.0209, -0.0602, -0.0172, -0.0178, -0.00286, 0.00755, 0.0390, 0.0578, 0.0768, 0.0817, 0.0604, 0.0310, 0.00235, 0.00653, -0.0414, -0.0460, -0.00806, 0.0171, 0.0578, 0.00948, -0.00561, 0.00449, -0.0125, -0.0288, -0.0446, -0.0359, -0.0260, -0.0113, 0.0209, 0.0529, 0.0871, 0.116, 0.0747, 0.0242, -0.0375, -0.0276, -0.0221, -0.0890, -0.0992, -0.0601, -0.0343, 0.00785, 0.0264, 0.0518, 0.0368, 0.00826, -0.00571, -0.0213, -0.0323, -0.0243, -0.0383, -0.0561, -0.0736, -0.0819, -0.0533, -0.0347, -0.00112, 0.00663, -0.00377, -0.000510, -0.0171, -0.0418, -0.00816, 0.00806, 0.0381, 0.0627, 0.0678, 0.0259, -0.00581, -0.0483, -0.0363, -0.0248, -0.00489, 0.0128, 0.0386, 0.0246, -0.0231, -0.0436, -0.0692, -0.0674, -0.0602, -0.0523, -0.0416, -0.0315, -0.0271, -0.0552, -0.0640, -0.0926, -0.113, -0.0898, -0.0785, -0.0593, -0.0482, -0.0340, -0.0203, 0.00204, 0.0215, 0.0441, 0.0625, 0.0782, 0.0951, 0.109, 0.115, 0.121, 0.127, 0.136, 0.162, 0.184, 0.208, 0.126, 0.0451, -0.0143, -0.0679, -0.0566, -0.0707, -0.100, -0.127, -0.120, -0.107, -0.0938, -0.0758, -0.0825, -0.0867, -0.0877, -0.0880, -0.0890, -0.0885, -0.0902, -0.0548, 0.00530, 0.0219, 0.0250, 0.0591, 0.0320, 0.0241, 0.0495, 0.0601, 0.0535, 0.0362, 0.0201, 0.0203, 0.0502, 0.0350, 0.0294, 0.0441, 0.0244, 0.00897, 0.00785, -0.0151, -0.00785, -0.00194, 0.00765, 0.00449, -0.0148, -0.0322, -0.0246, -0.00286, 0.0186, 0.0434, 0.0448, 0.0522, 0.0475, 0.0488, 0.0197, 0.0226, 0.0279, 0.0401, 0.0514, 0.0588, 0.0600, 0.0838, 0.0813, 0.0968, 0.0352, 0.00459, -0.0125, -0.0354, -0.0434, -0.0424, -0.0280, -0.0275, 0.00755, 0.0436, -0.0236, -0.0395, -0.00846, 0.0142, 0.0454, 0.00275, -0.0711, -0.0812, -0.0256, -0.0138, 0.00806, -0.0117, -0.0256, -0.0340, -0.0274, -0.0307, -0.0204, -0.00683, -0.00387, 0.0107, 0.0302, 0.0351, 0.0976, 0.0916, 0.0183, -0.0369, -0.101, -0.0823, -0.0759, -0.0550, -0.0337, -0.0131, 0.00316, 0.0151, 0.0518, -0.00224, -0.0499, -0.0365, -0.0705, -0.0526, -0.0378, 0.00897, 0.0644, 0.0858, 0.131, 0.142, 0.121, 0.0766, 0.0229, -0.00897, -0.0231, 0.00755, 0.0185, 0.0555, 0.0407, 0.00459, -0.00836, -0.0189, -0.00204, 0.000612, -0.0119, -0.0214, -0.0309, -0.0522, -0.0741, -0.0590, -0.0271, -0.0182, 0.00408, 0.00999, 0.0140, 0.0225, 0.0446, 0.00928, -0.0559, -0.0566, -0.0248, -0.00826, 0.0255, 0.0418, 0.0186, -0.00275, -0.0248, -0.00153, 0.0252, 0.0491, 0.0798, 0.0634, 0.0338, -0.00143, -0.0199, -0.0252, -0.0216, -0.0112, 0.00510, 0.0246, -0.00347, -0.0220, -0.0480, -0.0370, -0.0199, -0.00184, 0.0173, -0.00816, 0.000510, 0.0235, 0.0381, 0.0613, 0.0526, 0.0441, 0.0351, 0.0515, 0.0666, 0.0696, 0.0175, -0.0173, -0.0537, -0.0677, -0.0395, -0.0226, -0.00337, 0.0121, -0.0131, -0.0358, -0.0524, -0.0342, -0.0222, -0.00122, 0.0145, 0.00714, -0.00642, -0.0122, -0.0328, -0.0353, -0.00928, 0.00744, 0.0315, 0.0481, 0.0615, 0.0587, 0.0337, -0.00744, -0.0792, -0.0620, -0.0447, -0.0213, 0.00316, 0.0357, 0.0299, 0.0123, 0.0345, 0.0323, 0.0259, 0.0210, 0.0202, 0.0177, 0.00214, -0.0147, -0.0350, -0.0346, -0.0148, -0.00286, 0.0173, -0.00979, -0.0260, -0.0284, -0.0396, -0.0247, -0.0219, -0.0186, -0.0177, -0.00387, -0.00275, -0.0189, -0.0125, 0.00887, 0.0350, 0.0709, 0.0928, 0.0870, 0.0775, 0.0523, 0.0190, 0.00153, -0.0194, -0.0154, -0.00744, 0.00214, 0.0132, 0.0219, 0.00245, -0.0126, -0.0335, -0.0529, -0.0722, -0.0590, -0.0471, -0.0313, -0.0148, -0.000918, -0.0184, -0.0324, -0.0474, -0.0399, -0.0352, -0.0322, -0.0444, -0.0501, -0.0484, -0.0428, -0.0368, -0.0282, -0.0263, -0.0142, -0.00693, 0.0517, 0.0736, 0.0895, 0.0797, 0.0780, 0.0448, 0.00816, 0.00133, -0.0128, -0.00153, 0.00306, 0.0106, 0.0106, 0.0197, 0.0209, 0.00755, -0.00571, -0.00734, 0.00714, 0.0108, 0.0150, -0.000918, -0.0162, -0.0191, -0.000714, 0.0158, 0.0107, -0.0117, -0.0308, -0.0315, -0.00969, -0.00591, 0.000408, 0.00204, 0.00510, 0.00581, 0.00989, 0.0137, 0.0180, 0.0222, 0.0266, 0.0308, 0.0353, 0.0394, 0.0483, 0.0401, 0.0243, 0.0117, -0.00806, -0.0126, 0.00551, 0.00275, -0.0255, -0.0577, -0.0642, -0.0603, -0.0421, -0.00693, 0.0277, 0.0282, -0.00214, -0.00612, -0.0112, -0.0225, -0.0424, -0.0529, -0.0226, 0.00306, 0.00806, 0.0142, 0.0174, 0.0258, 0.0329, 0.0399, 0.0167, -0.0139, -0.0329, -0.0297, -0.0293, -0.0310, -0.0346, -0.0250, -0.00775, 0.0127, 0.0383, 0.0410, 0.0250, 0.0159, -0.00408, -0.0156, -0.0295, -0.0322, -0.0113, 0.00959, 0.0342, 0.0587, 0.0432, 0.0146, -0.000714, -0.0138, -0.0275, -0.0348, -0.0364, -0.0404, -0.0410, -0.0498, -0.0489, -0.0414, -0.0415, -0.0358, -0.0191, -0.00581, 0.00449, -0.00194, -0.00734, -0.0172, -0.0117, 0.0128, 0.0365, 0.0667, 0.0730, 0.0777, 0.0754, 0.0640, 0.0494, 0.0269, -0.00449, -0.0294, -0.0392, -0.0502, -0.0436, -0.0424, -0.0281, -0.00530, 0.0242, 0.0434, 0.0616, 0.0461, 0.0290, 0.0128, -0.00551, -0.0280, -0.0431, -0.0178, 0.000102, 0.0259, 0.0490, 0.0654, 0.0565, 0.0421, 0.0188, -0.00489, -0.0309, -0.0541, -0.0722, -0.0946, -0.0880, -0.0643, -0.0383, 0.00887, 0.0315, 0.0601, 0.0626, 0.0393, 0.0358, 0.0317, 0.0251, 0.00194, -0.0202, -0.0161, -0.00153, 0.0101, 0.0292, 0.0416, 0.0574, 0.0541, 0.0320, 0.0168, -0.00245, -0.0193, -0.0281, -0.0378, -0.0459, -0.0545, -0.0493, -0.0386, -0.0302, -0.0200, -0.0188, -0.0162, -0.0121, -0.00540, 0.000204, 0.00602, -0.00235, -0.0114, -0.0209, -0.0328, -0.0396, -0.0348, -0.0293, -0.0334, -0.0415, -0.0497, -0.0574, -0.0657, -0.0566, -0.0455, -0.000816, 0.0258, 0.0419, 0.0657, 0.0590, 0.0483, 0.0392, 0.0393, 0.0347, 0.0364, 0.000816, -0.0259, -0.0469, -0.0480, -0.0226, -0.00663, 0.0167, 0.0362, 0.0514, 0.0378, 0.0286, 0.0161, 0.00418, -0.00275, 0.00214, 0.00449, 0.0101, 0.0138, 0.00959, 0.00591, 0.00224, -0.00316, -0.00316, 0.000408, -0.00255, -0.0126, -0.0240, -0.0414, -0.0540, -0.0715, -0.0329, -0.00530, 0.00959, 0.0334, 0.0487, 0.0519, 0.0365, 0.0349, 0.0312, 0.0291, 0.0268, 0.0171, -0.000510, -0.0216, -0.0372, -0.0316, -0.0303, -0.0286, -0.0242, -0.0271, -0.0314, -0.0373, -0.0364, -0.0314, -0.0197, 0.00194, 0.0200, 0.0163, 0.0132, 0.0143, 0.0112, 0.0110, 0.00938, 0.00908, 0.00194, -0.0134, -0.0252, -0.0445, -0.0441, -0.0306, -0.0196, -0.00489, 0.00989, 0.0171, 0.0151, 0.0176, 0.00795, -0.00591, -0.0219, -0.0239, 0.00612, 0.0267, 0.0274, 0.00857, -0.00418, -0.0231, -0.00775, 0.00316, 0.0186, 0.0316, 0.0488, 0.0468, 0.0169, -0.00591, -0.0404, -0.0453, -0.0246, -0.0103, 0.0221, 0.0266, 0.0123, 0.000816, -0.0171, -0.0390, -0.0577, -0.0796, -0.0631, -0.0261, 0.00449, 0.0465, 0.0798, 0.112, 0.0972, 0.0499, 0.0124, -0.0395, -0.0863, -0.125, -0.0881, -0.0628, -0.0232, 0.0153, 0.0486, 0.0329, 0.0253, 0.00969, 0.0231, 0.0345, 0.0515, 0.0606, 0.0529, 0.0563, 0.0607, 0.0629, 0.0605, 0.0475, 0.0161, -0.00785, -0.0235, -0.0309, -0.0335, -0.0371, -0.0500, -0.0673, -0.0737, -0.0799, -0.0863, -0.0577, -0.0213, 0.0131, 0.0389, 0.0498, 0.0370, 0.0255, 0.0161, 0.0384, 0.0606, 0.0642, 0.0377, 0.0221, 0.00408, 0.00235, -0.00632, -0.0292, -0.0387, -0.0466, -0.0515, -0.0792, -0.0640, -0.0193, 0.0193, 0.0519, 0.0759, 0.0764, 0.0573, 0.0464, 0.0535, 0.0648, 0.0745, 0.0868, 0.0945, 0.0994, 0.0947, 0.0830, 0.0269, -0.0295, -0.0919, -0.139, -0.0876, -0.0477, 0.00826, 0.0606, 0.0839, 0.0567, 0.0307, -0.00112, -0.0387, -0.0354, -0.00204, 0.0184, 0.0645, 0.103, 0.114, 0.113, 0.0959, 0.0821, 0.0620, 0.0518, 0.0210, -0.0205, -0.0583, -0.104, -0.136, -0.125, -0.123, -0.103, -0.0777, -0.0559, -0.0499, -0.0413, -0.0297, -0.0179, -0.0144, -0.00999, -0.00510, 0.00194, 0.00816, 0.00357, -0.00296, -0.00622, -0.00347, -0.00133, 0.00775, 0.0209, 0.0333, 0.0459, 0.0588, 0.0573, 0.0473, 0.0306, 0.0109, -0.000408, -0.00337, -0.00704, -0.0103, -0.0205, -0.0192, 0.00255, 0.0137, 0.0248, 0.0271, 0.0270, 0.0158, -0.00194, -0.00969, -0.0220, -0.0170, -0.0122, -0.00653, -0.0132, -0.0166, -0.0197, -0.0247, -0.0241, -0.0178, -0.0126, -0.0189, -0.0270, -0.0329, -0.0343, -0.0463, -0.0438, -0.0341, -0.0217, -0.00704, 0.00306, 0.000306, -0.00948, -0.00908, -0.0153, -0.0167, -0.0243, -0.0329, -0.0429, -0.0466, -0.0405, -0.0356, -0.0263, -0.0175, -0.00204, 0.0159, 0.0290, 0.0369, 0.0361, 0.0274, 0.0103, -0.00459, -0.0127, -0.0250, -0.0234, -0.0128, -0.00693, 0.00184, 0.00948, 0.0204, 0.0292, 0.0372, 0.0317, 0.0185, 0.00245, -0.0159, -0.0325, -0.0223, -0.0120, 0.00153, 0.0156, 0.0304, 0.0248, 0.0139, 0.0104, 0.00245, -0.00102, -0.00235, -0.00367, -0.00489, -0.00908, -0.0158, -0.0151, -0.00775, 0.0, -0.00204, -0.0151, -0.0229, -0.0381, -0.0372, -0.0256, -0.0167, -0.00143, 0.0156, 0.0308, 0.0401, 0.0414, 0.0393, 0.0335, 0.0187, 0.0127, 0.00714, -0.000102, -0.00693, -0.0133, -0.0131, -0.0117, -0.0104, -0.00857, -0.0144, -0.0209, -0.0274, -0.0357, -0.0366, -0.0303, -0.0231, 0.00184, 0.0101, 0.0207, 0.0261, 0.0203, 0.0163, 0.0109, 0.0159, 0.0202, 0.0247, 0.0144, 0.00561, -0.00673, -0.00989, -0.00265, 0.00163, 0.0102, 0.00959, 0.00398, 0.0, -0.00673, -0.0101, -0.0111, -0.0117, -0.0134, -0.0171, -0.0203, -0.0106, -0.00102, 0.00561, 0.0132, 0.0204, 0.0203, 0.0172, 0.0146, 0.0111, 0.0131, 0.0155, 0.0140, 0.0119, 0.00989, 0.00612, -0.00286, -0.00510, -0.00857, -0.0182, -0.0327, -0.0428, -0.0528, -0.0481, -0.0402, -0.0296, -0.0112, 0.00510, 0.0135, 0.0160, 0.0180, 0.0199, 0.0260, 0.0338, 0.0334, 0.0257, 0.0176, 0.00459, -0.00867, -0.0194, -0.0234, -0.0310, -0.0282, -0.0231, -0.0177, -0.0123, -0.0128, -0.0132, -0.00816, -0.00265, 0.00398, 0.00663, 0.00561, 0.00510, 0.00653, 0.0128, 0.0183, 0.0248, 0.0313, 0.0304, 0.0256, 0.0220, 0.0166, 0.0196, 0.0239, 0.0288, 0.0307, 0.0200, 0.0108, -0.00398, -0.0166, -0.0328, -0.0342, -0.0223, -0.0151, -0.00102, 0.00112, -0.00540, -0.0103, -0.0128, -0.0146, -0.0132, -0.0106, -0.00724, -0.00184, 0.00337, 0.00867, 0.0161, 0.0244, 0.0325, 0.0348, 0.0324, 0.0217, 0.00744, 0.000918, -0.00153, -0.00479, -0.00765, -0.0123, -0.0159, -0.0118, -0.00571, -0.000612, -0.000306, -0.000306, 0.00143, 0.00469, 0.00734, 0.00877, 0.00999, 0.0126, 0.0150, 0.0175, 0.0204, 0.0261, 0.0323, 0.0293, 0.0236, 0.0107, -0.000102, -0.00112, -0.00367, -0.00540, -0.00846, -0.00530, -0.000714, 0.00377, 0.00979, 0.0158, 0.0209, 0.0146, 0.00744, 0.00214, -0.00224, -0.00714, -0.0102, -0.00765, -0.00551, -0.00296, -0.00153, 0.0, -0.000102, -0.000612, -0.00112, -0.00102, -0.000306, 0.000102, 0.00163, 0.00540, 0.00877, 0.0128, 0.0157, 0.0134, 0.0104, 0.00571, 0.000612, -0.00408, -0.00999, -0.00979, -0.00469, -0.000714, 0.00316, 0.00459, 0.00693, 0.00530, 0.00459, 0.00184, -0.000204, -0.00296, -0.00286, -0.00173, -0.000612, -0.000510, -0.00173, -0.00224, -0.00367, -0.00326, -0.000714, 0.00143, 0.00418, 0.00663, 0.00642, 0.00530, 0.000612, -0.00459, -0.00775, -0.00653, -0.00663, -0.0109, -0.0164, -0.0177, -0.0119, -0.00714, -0.00367, -0.00255, -0.000204, 0.000408, 0.00367, 0.00836, 0.0117, 0.00714, 0.00387, -0.00102, -0.00255, -0.00347, -0.00418, -0.00459, -0.00663, -0.0122, -0.0119, -0.0110, -0.00999, -0.00826, -0.00775, -0.0112, -0.0141, -0.0169, -0.0147, -0.0133, -0.0113, -0.0110, -0.00999, -0.00999, -0.00642, -0.000204, 0.00551, 0.0112, 0.0155, 0.0201, 0.0173, 0.0143, 0.0105, 0.00653, 0.00102, -0.00683, -0.0107, -0.0124, -0.0141, -0.0169, -0.0219, -0.0274, -0.0279, -0.0221, -0.0182, -0.0118, -0.00622, 0.0, 0.00551, 0.0101, 0.0147, 0.0191, 0.0237, 0.0267, 0.0260, 0.0231, 0.0174, 0.0130, 0.00551, -0.00479, -0.0152, -0.0214, -0.0259, -0.0257, -0.0250, -0.0230, -0.0183, -0.0139, -0.00836, -0.00143, 0.00449, 0.00908, 0.00989, 0.0102, 0.0102, 0.0106, 0.0139, 0.0123, 0.0102, 0.00734, 0.00194, -0.00286, -0.00846, -0.0134, -0.0194, -0.0198, -0.0142, -0.0102, -0.00449, -0.00449, -0.00408, -0.00459, -0.00500, -0.00398, -0.00214, -0.00224, -0.00163, -0.00163, -0.00122, -0.00122, 0.00286, 0.00693, 0.0117, 0.00979, 0.00571, 0.00173, -0.00316, -0.00795, -0.00938, -0.00938, -0.00969, -0.00938, -0.00877, -0.00530, -0.00224, 0.00143, 0.00387, 0.00347, 0.00347, 0.00296, 0.00408, 0.00551, 0.00683, 0.00867, 0.00887, 0.00785, 0.00714, 0.00622, 0.00765, 0.00887, 0.0104, 0.0113, 0.00918, 0.00765, 0.00540, 0.00632, 0.00806, 0.00989, 0.0120, 0.0138, 0.0107, 0.00785, 0.00449, 0.00122, -0.00214, -0.00428, -0.00693, -0.00887, -0.0122, -0.0162, -0.0202, -0.0238, -0.0263, -0.0275, -0.0283, -0.0293, -0.0283, -0.0275, -0.0263, -0.0256, -0.0223, -0.0176, -0.0132, -0.0111, -0.0107, -0.00969, -0.00928, -0.00734, -0.00602, -0.00408, -0.00235, 0.00184, 0.00357, 0.00428, 0.00489, 0.00438, 0.00438, 0.00387, 0.00367, 0.00326, 0.00479, 0.00642, 0.00867, 0.0112, 0.0137, 0.0163, 0.0185, 0.0176, 0.0171, 0.0162, 0.0150, 0.0110, 0.00693, 0.00326, 0.000918, 0.00194, 0.00286, 0.00408, 0.00551, 0.00704, 0.00969, 0.0119, 0.0146, 0.0146, 0.0127, 0.0124, 0.0126, 0.0128, 0.0133, 0.0136, 0.0108, 0.00806, 0.00469, 0.00204, 0.000510, -0.00112, -0.00255, -0.00112, 0.00184, 0.00438, 0.00775, 0.0105, 0.0136, 0.0103, 0.00632, 0.00184, -0.00275, -0.00816, -0.00908, -0.00551, -0.00306, 0.000612, 0.00122, 0.00224, 0.00275, 0.00326, 0.00489, 0.00704, 0.00908, 0.0114, 0.0136, 0.0143, 0.0140, 0.0143, 0.0138, 0.0142, 0.0104, 0.00306, -0.00326, -0.00642, -0.00867, -0.0114, -0.0160, -0.0201, -0.0192, -0.0186, -0.0173, -0.0159, -0.0145, -0.0132, -0.0127, -0.0122, -0.0118, -0.0116, -0.0108, -0.00653, -0.00214, 0.000204, -0.000408, -0.000408, -0.00143, -0.00194, -0.000816, 0.00112, 0.00275, 0.00510, 0.00540, 0.00428, 0.00306, 0.00112, -0.000510, -0.00245, -0.00204, -0.000714, 0.000408, 0.00194, 0.00337, 0.00489, 0.00581, 0.00663, 0.00755, 0.00826, 0.00928, 0.0130, 0.0172, 0.0147, 0.0107, 0.00714, 0.00357, 0.000204, -0.00326, -0.00653, -0.00969, -0.00928, -0.00846, -0.00744, -0.00520, -0.00306, -0.000408, 0.00337, 0.00530, 0.00449, 0.00408, 0.00306, 0.00204, 0.00102, 0.000102, -0.00102, -0.00143, 0.000204, 0.00163, 0.00306, 0.00235, 0.00163, 0.00163, 0.00265, 0.00326, 0.00459, 0.00469, 0.00214, -0.000306, -0.00306, -0.00612, -0.00887, -0.0118, -0.0109, -0.00959, -0.00795, -0.00693, -0.00642, -0.00581, -0.00510, -0.00418, -0.00337, -0.00245, -0.00143, -0.000510, 0.000408, 0.00133, 0.00214, 0.000102, -0.00184, -0.00408, -0.00653, -0.00887, -0.0112, -0.0132, -0.0154, -0.0168, -0.0177, -0.0188, -0.0198, -0.0206, -0.0217, -0.0202, -0.0176, -0.0138, -0.00663, -0.00143, 0.00204, 0.00561, 0.00836, 0.00908, 0.0104, 0.00948, 0.00816, 0.00622, 0.00337, 0.000306, -0.00245, -0.00581, -0.00459, -0.00133, 0.00133, 0.00337, 0.00489, 0.00551, 0.00591, 0.00632, 0.00663, 0.00704, 0.00734, 0.00765, 0.00785, 0.00744, 0.00704, 0.00653, 0.00591, 0.00551, 0.00489, 0.00622, 0.00806, 0.00979, 0.0116, 0.0128, 0.0142, 0.0134, 0.0125, 0.0116, 0.0105, 0.00887, 0.00734, 0.00571, 0.00408, 0.00245, 0.000714, 0.000408, 0.000816, 0.000918, 0.00235, 0.00428, 0.00530, 0.00622, 0.00734, 0.00806, 0.00908, 0.00969, 0.00969, 0.00948, 0.00908, 0.00255, 0.0, -0.000510, -0.00204, -0.00245, -0.00337, -0.00489, -0.00653, -0.00816, -0.00989, -0.0116, -0.0131, -0.0108, -0.00908, -0.00673, -0.00438, -0.00194, -0.00112, -0.000918, -0.000510, -0.000408, 0.000510, 0.00143, 0.00245, 0.00357, 0.00438, 0.00449, 0.00398, 0.00133, -0.000816, -0.00367, -0.00479, -0.00561, -0.00632, -0.00693, -0.00744, -0.00816, -0.00765, -0.00653, -0.00286, 0.000510, 0.00469, 0.00663, 0.00612, 0.00612, 0.00551, 0.00500, 0.00449, 0.00398, 0.00296, 0.0, -0.00265, -0.00530, -0.00428, -0.00377, -0.00255, -0.00153, -0.000204, 0.000204, 0.0, -0.000102, -0.000408, -0.000714, -0.000612, -0.000102, 0.000306, 0.000714, -0.000102, -0.000714, -0.00153, -0.00245, -0.00296, -0.00286, -0.00286, -0.00255, -0.00245, -0.00214, -0.00275, -0.00449, -0.00591, -0.00775, -0.00857, -0.00836, -0.00846, -0.00765, -0.00510, -0.00286, -0.000204, 0.00184, 0.00184, 0.00214, 0.00214, 0.00214, 0.00316, 0.00438, 0.00500, 0.00500, 0.00500, 0.00489, 0.00479, 0.00530, 0.00683, 0.00806, 0.00969, 0.00989, 0.00979, 0.00979, 0.00948, 0.00928, 0.00908, 0.00887, 0.00857, 0.00836, 0.00816, 0.00785, 0.00632, 0.00479, 0.00357, 0.00367, 0.00347, 0.00357, 0.00367, 0.00326, 0.00194, 0.00153, 0.00143, 0.00122, 0.00122, 0.00112, 0.0, -0.00102, -0.00204, -0.00255, -0.00296, -0.00337, -0.00377, -0.00489, -0.00642, -0.00785, -0.00938, -0.0109, -0.0124, -0.0141, -0.0146, -0.0122, -0.0105, -0.00795, -0.00683, -0.00642, -0.00591, -0.00571, -0.00540, -0.00510, -0.00489, -0.00459, -0.00428, -0.00398, -0.00316, -0.00265, -0.00112, 0.00102, 0.00316, 0.00469, 0.00489, 0.00551, 0.00561, 0.00642, 0.00857, 0.0103, 0.00857, 0.00724, 0.00540, 0.00337, 0.00286, 0.00296, 0.00286, 0.00275, 0.00122, -0.000102, -0.00102, -0.00112, -0.00163, -0.00153, -0.00316, -0.00693, -0.0101, -0.0104, -0.0107, -0.0108, -0.0101, -0.00908, -0.00806, -0.00755, -0.00734, -0.00693, -0.00683, -0.00642, -0.00408, -0.00133, 0.00133, 0.00438, 0.00724, 0.0101, 0.00877, 0.00642, 0.00418, 0.00133, -0.00143, -0.00357, -0.00377, -0.00449, -0.00469, -0.00500, -0.00520, -0.00551, -0.00571, -0.00591, -0.00469, -0.00296, -0.00133, 0.000714, 0.00173, 0.00194, 0.00255, 0.00255, 0.00357, 0.00510, 0.00653, 0.00816, 0.00959, 0.00989, 0.0104, 0.0107, 0.0110, 0.0112, 0.0106, 0.0101, 0.00948, 0.00877, 0.00806, 0.00734, 0.00642, 0.00561, 0.00469, 0.00387, 0.00316, 0.00275, 0.00214, 0.00173, 0.00122, 0.000714, 0.000306, 0.000816, 0.00133, 0.00194, 0.00224, 0.000918, -0.000306, -0.00173, -0.00316, -0.00479, -0.00540, -0.00489, -0.00438, -0.00387, -0.00255, -0.000918, 0.000714, 0.00173, 0.00224, 0.00357, 0.00622, 0.00846, 0.0114, 0.0109, 0.00887, 0.00724, 0.00479, 0.00275, 0.000408, -0.00163, -0.00245, -0.00337, -0.00469, -0.00704, -0.00857, -0.00897, -0.00948, -0.00969, -0.00989, -0.0101, -0.0103, -0.0104, -0.00999, -0.00969, -0.00918, -0.00887, -0.00908, -0.00928, -0.00948, -0.00979, -0.00989, -0.00897, -0.00826, -0.00724, -0.00693, -0.00693, -0.00683, -0.00683, -0.00673, -0.00673, -0.00653, -0.00571, -0.00510, -0.00438, -0.00347, -0.00235, -0.00133, -0.000204, 0.00102, 0.00173, 0.000918, 0.000306, -0.000510, -0.00143, -0.000714, 0.000612, 0.00163, 0.00133, 0.00122, 0.000816, 0.000510, 0.0, 0.000204, 0.000918, 0.00143, 0.00214, 0.00224, 0.00102, 0.0, -0.00122, -0.00245, -0.00245, -0.00194, -0.00163, -0.00184, -0.00224, -0.00275, -0.00306, -0.00265, -0.00224, -0.00224, -0.00326, -0.00418, -0.00520, -0.00469, -0.00438, -0.00387, -0.00326, -0.00265, -0.00204, -0.00194, -0.00184, -0.00153, 0.00133, 0.00418, 0.00683, 0.00714, 0.00775, 0.00795, 0.00816, 0.00826, 0.00857, 0.00795, 0.00622, 0.00489, 0.00316, 0.00296, 0.00306, 0.00326, 0.00357, 0.00377, 0.00245, 0.000714, -0.000816, -0.00286, -0.00357, -0.00224, -0.00133, 0.000102, 0.000306, 0.000102, -0.000102, -0.000510, -0.000816, -0.00112, -0.00153, -0.00122, -0.000816, -0.000408, 0.000102, 0.000510, 0.000816, 0.000816, 0.000918, 0.00102, 0.00224, 0.00337, 0.00469, 0.00602, 0.00734, 0.00806, 0.00632, 0.00510, 0.00326, 0.00163, -0.000408, 0.0, 0.00306, 0.00530, 0.00887, 0.00836, 0.00316, -0.000204, -0.00632, -0.00286, 0.00591, 0.0133, 0.0225, 0.0189, 0.00959, 0.00928, 0.00785, 0.00918, 0.00163, -0.00857, -0.0204, -0.0151, -0.000714, 0.0109, 0.0101, 0.00489, 0.000306, -0.00704, -0.0109, -0.00153, 0.00704, 0.0141, 0.00683, 0.00204, 0.00357, 0.00918, 0.00877, 0.00530, 0.00245, 0.00163, 0.00133, -0.00112, -0.00347, -0.00683, -0.00551, -0.00204, 0.00143, 0.00489, 0.00989, 0.00591, -0.00489, -0.0121, -0.00969, -0.00612, -0.00245, 0.00326, 0.00387, 0.00143, -0.000102, -0.000714, 0.000612, 0.00102, 0.00245, 0.00133, 0.000408, -0.000816, -0.00275, -0.00561, -0.00857, -0.00948, -0.00622, -0.00367, -0.000204, 0.00337, 0.00377, -0.00143, -0.00459, -0.0105, -0.00785, -0.00235, 0.00316, 0.00418, 0.00265, 0.00133, 0.00153, 0.00622, 0.00744, 0.00755, 0.00775, 0.00744, 0.00724, 0.00581, 0.00387, 0.00214, -0.000204, -0.00337, -0.00653, -0.00693, -0.00520, -0.00551, -0.00551, -0.00581, -0.00337, 0.0, 0.00337, 0.00714, 0.00979, 0.00867, 0.00816, 0.00693, 0.00765, 0.00785, 0.00867, 0.00622, 0.00377, 0.000714, -0.000102, 0.000408, 0.000510, -0.00102, -0.00296, -0.00469, -0.00704, -0.00663, -0.00449, -0.00286, -0.000714, -0.00235, -0.00337, -0.00520, -0.00612, -0.00571, -0.00551, -0.00510, -0.00673, -0.00928, -0.0116, -0.0143, -0.0141, -0.0143, -0.0123, -0.00979, -0.00755, -0.00469, -0.00347, -0.00428, -0.00459, -0.00561, -0.00642, -0.00734, -0.00816, -0.00744, -0.00530, -0.00377, -0.00265, -0.00143
])

dt = 0.02
t_force = np.arange(0, len(force_data) * dt, dt)

# Scale the force data to get reasonable displacements
force_scale = 1000.0  # Scale factor for force
force_data_scaled = force_data * force_scale

# Interpolation function for forcing
def forcing(t):
    # Handle both scalar and array inputs
    t = np.asarray(t)
    return np.interp(t, t_force, force_data_scaled, left=force_data_scaled[0], right=force_data_scaled[-1])

# ------------------------------------------------------------
# 2. Generate "True" Solution Using ODEINT
# ------------------------------------------------------------
def sdof_ode(y, t):
    x, v = y
    dxdt = v
    dvdt = (forcing(t) - c*v - k*x) / m
    return [dxdt, dvdt]

# Adjust time domain to match force data duration
t_max = len(force_data) * dt
t_true = np.linspace(0, t_max, 2000)
y0 = [0.0, 0.0]
true_sol = odeint(sdof_ode, y0, t_true)

x_true = true_sol[:, 0]
v_true = true_sol[:, 1]

print(f"True displacement range: {x_true.min():.2e} to {x_true.max():.2e}")
print(f"True displacement max absolute: {np.max(np.abs(x_true)):.2e}")

plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.plot(t_true, x_true)
plt.title("True Displacement")
plt.xlabel("t")
plt.ylabel("x(t)")
plt.grid()

plt.subplot(2, 2, 2)
plt.plot(t_force, force_data_scaled)
plt.title("Scaled Forcing Function")
plt.xlabel("t")
plt.ylabel("f(t)")
plt.grid()

plt.subplot(2, 2, 3)
plt.plot(t_force, force_data)
plt.title("Original Forcing Function")
plt.xlabel("t")
plt.ylabel("f(t) (original)")
plt.grid()

plt.tight_layout()
plt.show()

# ------------------------------------------------------------
# 3. PINN Model with Scaled Output
# ------------------------------------------------------------
class PINN(nn.Module):
    def __init__(self, output_scale=1e-8):
        super().__init__()
        self.output_scale = output_scale
        self.net = nn.Sequential(
            nn.Linear(1, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.Tanh(),
            nn.Linear(128, 1)
        )

    def forward(self, t):
        return self.net(t) * self.output_scale

# Use a larger network and scale output appropriately
model = PINN(output_scale=1e-8)

# ------------------------------------------------------------
# 4. Physics Residual m*x'' + c*x' + k*x - f(t) = 0
# ------------------------------------------------------------
def physics_loss(model, t):
    t.requires_grad_(True)

    x = model(t)

    # First derivative
    x_t = grad(x, t, torch.ones_like(x), create_graph=True)[0]

    # Second derivative
    x_tt = grad(x_t, t, torch.ones_like(x_t), create_graph=True)[0]

    # Convert time to numpy for interpolation, then back to tensor
    t_np = t.detach().numpy().flatten()
    f_np = forcing(t_np)
    f_t = torch.tensor(f_np, dtype=torch.float32).view(-1, 1)

    res = m * x_tt + c * x_t + k * x - f_t
    return torch.mean(res**2)

# ------------------------------------------------------------
# 5. Initial Condition Loss
# ------------------------------------------------------------
t0 = torch.tensor([[0.0]], requires_grad=True)
x0 = torch.tensor([[0.0]])  # x(0)
v0 = torch.tensor([[0.0]])  # x'(0)

def ic_loss(model):
    t0.requires_grad_(True)

    x_pred = model(t0)
    x_t_pred = grad(x_pred, t0, torch.ones_like(x_pred), create_graph=True)[0]

    lx = (x_pred - x0).pow(2).mean()
    lv = (x_t_pred - v0).pow(2).mean()

    return lx + lv

# ------------------------------------------------------------
# 6. Training Setup with Better Optimization
# ------------------------------------------------------------
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=500, factor=0.5)

# Use more training points
t_train = torch.linspace(0, t_max, 500).view(-1, 1)
t_train.requires_grad_(True)

# ------------------------------------------------------------
# 7. Training Loop with Progress Monitoring
# ------------------------------------------------------------
loss_history = []

for epoch in range(10000):
    optimizer.zero_grad()

    loss_p = physics_loss(model, t_train)
    loss_ic = ic_loss(model)

    loss = loss_p + loss_ic
    loss.backward()

    # Gradient clipping to prevent explosions
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    optimizer.step()
    scheduler.step(loss)

    loss_history.append(loss.item())

    if epoch % 1000 == 0:
        current_lr = optimizer.param_groups[0]['lr']
        print(f"Epoch {epoch} | Loss = {loss.item():.6e} | LR = {current_lr:.2e}")

# ------------------------------------------------------------
# 8. Plot Results with Better Visualization
# ------------------------------------------------------------
t_tensor = torch.tensor(t_true).view(-1, 1).float()
x_pinn = model(t_tensor).detach().numpy().flatten()

plt.figure(figsize=(15, 10))

plt.subplot(2, 3, 1)
plt.plot(t_true, x_true, 'k-', label="True", linewidth=2)
plt.plot(t_true, x_pinn, 'r--', label="PINN Prediction", linewidth=1.5)
plt.xlabel("Time")
plt.ylabel("x(t)")
plt.title("Displacement Comparison")
plt.legend()
plt.grid()

plt.subplot(2, 3, 2)
plt.plot(t_force, force_data_scaled, 'g-', linewidth=1.5)
plt.xlabel("Time")
plt.ylabel("f(t)")
plt.title("Scaled Forcing Function")
plt.grid()

plt.subplot(2, 3, 3)
error = np.abs(x_true - x_pinn)
plt.plot(t_true, error, 'r-', linewidth=1.5)
plt.xlabel("Time")
plt.ylabel("Absolute Error")
plt.title("PINN Prediction Error")
plt.grid()

plt.subplot(2, 3, 4)
plt.semilogy(t_true, error, 'r-', linewidth=1.5)
plt.xlabel("Time")
plt.ylabel("Absolute Error (log scale)")
plt.title("PINN Prediction Error (Log Scale)")
plt.grid()

plt.subplot(2, 3, 5)
plt.semilogy(loss_history)
plt.xlabel("Epoch")
plt.ylabel("Loss (log scale)")
plt.title("Training Loss History")
plt.grid()

plt.subplot(2, 3, 6)
# Zoom in on a portion to see details
zoom_start = 100
zoom_end = 500
plt.plot(t_true[zoom_start:zoom_end], x_true[zoom_start:zoom_end], 'k-', label="True", linewidth=2)
plt.plot(t_true[zoom_start:zoom_end], x_pinn[zoom_start:zoom_end], 'r--', label="PINN", linewidth=1.5)
plt.xlabel("Time")
plt.ylabel("x(t)")
plt.title("Zoomed Displacement Comparison")
plt.legend()
plt.grid()

plt.tight_layout()
plt.show()

# Print detailed error statistics
final_error = np.mean(error)
max_error = np.max(error)
relative_error = np.mean(np.abs(error / (x_true + 1e-12)))  # Avoid division by zero

print(f"\nFinal Results:")
print(f"Mean absolute error: {final_error:.6e}")
print(f"Maximum absolute error: {max_error:.6e}")
print(f"Mean relative error: {relative_error:.6e}")
print(f"True displacement range: {x_true.min():.6e} to {x_true.max():.6e}")
print(f"PINN displacement range: {x_pinn.min():.6e} to {x_pinn.max():.6e}")

Result: 
True displacement range: -3.49e-05 to 3.48e-05
True displacement max absolute: 3.49e-05

Epoch 0 | Loss = 2.017914e+03 | LR = 1.00e-04
Epoch 1000 | Loss = 1.980983e+03 | LR = 1.00e-04
Epoch 2000 | Loss = 1.974087e+03 | LR = 1.00e-04
Epoch 3000 | Loss = 1.968799e+03 | LR = 1.00e-04
Epoch 4000 | Loss = 1.965494e+03 | LR = 1.00e-04
Epoch 5000 | Loss = 1.963384e+03 | LR = 1.00e-04
Epoch 6000 | Loss = 1.964455e+03 | LR = 1.00e-04
Epoch 7000 | Loss = 1.962327e+03 | LR = 2.50e-05
Epoch 8000 | Loss = 1.962300e+03 | LR = 6.25e-06
Epoch 9000 | Loss = 1.962295e+03 | LR = 1.56e-06


Final Results:
Mean absolute error: 1.264908e-05
Maximum absolute error: 3.494504e-05
Mean relative error: 1.990027e+02
True displacement range: -3.493918e-05 to 3.480026e-05
PINN displacement range: -3.995999e-07 to 3.950829e-07
