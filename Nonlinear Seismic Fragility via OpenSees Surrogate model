Performs nonlinear time-history analysis in OpenSees (Steel01, Newmark integration) to generate peak displacement response data and trains a neural-network surrogate with quantified accuracy and performance metrics.
Uses IDA, Monte Carlo simulation, and bounded lognormal MLE to derive smooth, marker-free seismic fragility curves for multiple damage states with full-scale, publication-quality plots.

Code:
#   FULL FRAGILITY ANALYSIS WITH PERFORMANCE MATRIX + SMOOTH CURVES
#   USING OPENSEES (STEEL01) — FULL CLEAN PLOTS
#   EXACT PIPELINE MATCHED TO CODE 2 (EXCEPT OPENSEES SOLVER)

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from scipy.stats import norm
from scipy.optimize import minimize
import openseespy.opensees as ops
import warnings
warnings.filterwarnings("ignore")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
np.random.seed(42)
torch.manual_seed(42)

print("="*90)
print(" FULL FRAGILITY ANALYSIS (OPENSEES STEEL01, SMOOTH CURVES, PERFORMANCE METRICS)")
print("="*90)


################################################################################
# GROUND MOTION — SAME AS CODE 2
################################################################################

AG = np.array([
-0.0110, -0.0103, -0.00897, -0.00969, -0.0122, -0.0145, -0.0131, -0.0112, -0.00867, -0.00867, -0.0134, -0.0179, -0.0198, -0.0165, -0.0147, -0.0110, -0.00836, -0.00428, -0.00673, -0.0134, -0.0194, -0.0200, -0.00673, 0.00306, 0.0144, -0.00500, -0.0131, -0.0147, -0.0207, -0.0265, -0.0331, -0.0312, -0.0175, -0.0201, -0.0166, -0.0167, -0.00683, 0.00255, 0.0153, 0.0241, 0.0257, 0.0343, 0.0472, 0.0502, 0.0427, 0.0366, 0.0276, 0.0240, 0.0346, 0.0420, 0.0540, 0.0652, 0.0746, 0.0665, 0.0611, 0.0408, 0.0408, 0.00642, -0.0525, -0.0803, -0.0615, -0.0494, -0.0255, -0.00602, 0.0137, 0.0314, 0.0509, 0.0724, 0.101, 0.124, 0.156, 0.148, 0.118, 0.0953, 0.0910, 0.0944, 0.0856, 0.0919, 0.101, 0.123, 0.0334, -0.151, -0.211, -0.203, -0.207, -0.186, -0.176, -0.178, -0.178, -0.185, -0.166, -0.138, -0.111, -0.0797, -0.0437, -0.00173, 0.0367, 0.0800, 0.118, 0.163, 0.200, 0.246, 0.278, 0.310, 0.326, 0.349, 0.288, 0.237, -0.122, -0.242, -0.167, -0.191, -0.112, -0.0768, -0.0176, 0.0115, 0.0544, 0.0913, 0.121, 0.179, 0.0587, -0.268, -0.158, -0.176, -0.103, -0.0590, 0.0242, -0.0683, -0.202, -0.167, -0.172, -0.151, -0.125, -0.102, -0.0766, -0.0533, -0.0276, -0.00449, 0.0192, -0.00969, -0.0442, -0.0855, -0.0970, -0.0730, -0.0611, -0.0341, -0.0110, 0.0189, 0.0428, 0.0686, -0.00989, -0.0379, -0.00408, 0.00112, 0.0351, 0.0576, 0.0900, 0.115, 0.139, 0.0223, 0.0246, 0.0696, 0.0703, 0.135, 0.138, 0.208, -0.0949, -0.134, -0.0706, -0.0557, 0.00734, 0.0688, -0.109, -0.152, -0.109, -0.118, -0.0777, -0.0570, -0.0219, -0.0128, -0.0687, -0.0330, -0.0344, -0.0111, 0.00173, 0.0305, 0.0498, 0.0620, 0.0226, -0.00326, -0.0250, 0.00785, 0.0215, 0.0579, 0.0842, 0.123, 0.151, 0.177, 0.0429, 0.00296, 0.0264, 0.0299, -0.00561, -0.0150, 0.0146, 0.0210, 0.0509, 0.0658, 0.0976, 0.115, 0.148, 0.166, 0.199, 0.190, 0.202, 0.180, 0.127, -0.123, -0.0553, -0.0392, -0.0317, -0.114, -0.169, -0.251, -0.207, -0.188, -0.135, -0.0979, -0.0331, 0.0157, 0.0832, 0.135, 0.186, -0.00591, -0.0172, 0.0291, 0.0456, 0.100, 0.145, 0.189, 0.251, 0.172, -0.141, -0.102, -0.111, -0.0925, -0.0478, -0.127, -0.215, -0.165, -0.172, -0.134, -0.113, -0.0788, -0.0520, -0.0555, -0.122, -0.123, -0.118, -0.117, -0.0731, -0.0557, 0.00653, -0.0820, -0.166, -0.0876, -0.0980, -0.0404, -0.0150, 0.0325, 0.0661, 0.0893, 0.0481, 0.0202, -0.00275, 0.0298, 0.0454, 0.0800, 0.105, 0.138, 0.164, 0.190, 0.131, 0.0653, 0.0208, 0.0320, 0.0380, 0.0506, 0.0240, -0.00857, -0.0171, -0.0115, -0.0234, -0.0253, -0.0160, -0.00704, 0.0150, 0.0386, 0.0590, 0.0260, -0.00418, -0.0436, -0.0136, 0.00969, 0.0235, -0.0132, -0.00510, 0.00816, 0.0214, 0.0387, 0.0520, 0.0160, -0.00326, -0.0113, 0.000510, 0.00775, 0.00357, -0.00969, -0.00367, -0.00163, 0.00387, 0.00867, -0.00571, -0.0310, -0.0429, -0.0249, -0.0241, -0.0180, -0.0132, -0.00184, 0.0207, -0.0110, -0.00928, -0.00347, -0.0108, -0.0113, -0.0101, -0.000204, 0.00744, 0.0240, 0.0362, 0.0719, 0.0794, 0.0188, -0.0268, -0.0126, -0.00428, 0.0162, 0.00489, -0.0223, -0.0476, -0.0436, -0.0220, -0.00438, 0.0162, 0.0326, 0.0427, 0.0125, -0.0163, -0.0208, -0.00836, -0.0210, -0.0140, -0.00561, 0.00540, 0.0137, 0.0271, 0.0237, 0.00806, -0.000816, 0.0204, 0.0444, 0.0502, 0.0195, 0.00938, -0.00224, -0.00214, 0.00530, 0.00948, 0.0260, 0.0375, 0.0535, 0.0552, 0.0433, 0.0406, 0.0570, 0.0771, 0.0372, 0.0419, 0.00999, -0.0208, -0.0254, -0.0413, -0.0421, -0.0480, -0.0442, -0.0467, -0.00581, 0.0182, -0.0212, -0.0502, -0.0540, -0.0369, -0.0413, -0.0314, -0.0322, -0.0270, -0.0270, -0.0274, -0.0352, -0.0315, -0.0221, -0.00795, 0.00887, 0.0287, 0.0316, 0.0365, 0.0348, 0.0365, 0.0293, 0.0311, 0.0114, 0.0218, 0.0139, 0.0392, -0.0878, -0.138, -0.137, -0.138, -0.121, -0.106, -0.0845, -0.0664, -0.0453, -0.0263, -0.00612, -0.00928, -0.0186, -0.0150, 0.00867, 0.0166, 0.00510, 0.0269, 0.0593, 0.0884, 0.122, 0.173, 0.113, -0.112, -0.0373, -0.0454, -0.0241, -0.0979, -0.0669, -0.0609, -0.0683, -0.0563, -0.00275, 0.0385, 0.109, 0.170, 0.0966, 0.0416, 0.0680, 0.0135, -0.00969, -0.0530, -0.0843, -0.117, -0.117, -0.0819, -0.0376, 0.00296, 0.0556, 0.120, 0.164, -0.0275, 0.00347, -0.00571, 0.00204, 0.0149, 0.0548, 0.0814, -0.0209, -0.0602, -0.0172, -0.0178, -0.00286, 0.00755, 0.0390, 0.0578, 0.0768, 0.0817, 0.0604, 0.0310, 0.00235, 0.00653, -0.0414, -0.0460, -0.00806, 0.0171, 0.0578, 0.00948, -0.00561, 0.00449, -0.0125, -0.0288, -0.0446, -0.0359, -0.0260, -0.0113, 0.0209, 0.0529, 0.0871, 0.116, 0.0747, 0.0242, -0.0375, -0.0276, -0.0221, -0.0890, -0.0992, -0.0601, -0.0343, 0.00785, 0.0264, 0.0518, 0.0368, 0.00826, -0.00571, -0.0213, -0.0323, -0.0243, -0.0383, -0.0561, -0.0736, -0.0819, -0.0533, -0.0347, -0.00112, 0.00663, -0.00377, -0.000510, -0.0171, -0.0418, -0.00816, 0.00806, 0.0381, 0.0627, 0.0678, 0.0259, -0.00581, -0.0483, -0.0363, -0.0248, -0.00489, 0.0128, 0.0386, 0.0246, -0.0231, -0.0436, -0.0692, -0.0674, -0.0602, -0.0523, -0.0416, -0.0315, -0.0271, -0.0552, -0.0640, -0.0926, -0.113, -0.0898, -0.0785, -0.0593, -0.0482, -0.0340, -0.0203, 0.00204, 0.0215, 0.0441, 0.0625, 0.0782, 0.0951, 0.109, 0.115, 0.121, 0.127, 0.136, 0.162, 0.184, 0.208, 0.126, 0.0451, -0.0143, -0.0679, -0.0566, -0.0707, -0.100, -0.127, -0.120, -0.107, -0.0938, -0.0758, -0.0825, -0.0867, -0.0877, -0.0880, -0.0890, -0.0885, -0.0902, -0.0548, 0.00530, 0.0219, 0.0250, 0.0591, 0.0320, 0.0241, 0.0495, 0.0601, 0.0535, 0.0362, 0.0201, 0.0203, 0.0502, 0.0350, 0.0294, 0.0441, 0.0244, 0.00897, 0.00785, -0.0151, -0.00785, -0.00194, 0.00765, 0.00449, -0.0148, -0.0322, -0.0246, -0.00286, 0.0186, 0.0434, 0.0448, 0.0522, 0.0475, 0.0488, 0.0197, 0.0226, 0.0279, 0.0401, 0.0514, 0.0588, 0.0600, 0.0838, 0.0813, 0.0968, 0.0352, 0.00459, -0.0125, -0.0354, -0.0434, -0.0424, -0.0280, -0.0275, 0.00755, 0.0436, -0.0236, -0.0395, -0.00846, 0.0142, 0.0454, 0.00275, -0.0711, -0.0812, -0.0256, -0.0138, 0.00806, -0.0117, -0.0256, -0.0340, -0.0274, -0.0307, -0.0204, -0.00683, -0.00387, 0.0107, 0.0302, 0.0351, 0.0976, 0.0916, 0.0183, -0.0369, -0.101, -0.0823, -0.0759, -0.0550, -0.0337, -0.0131, 0.00316, 0.0151, 0.0518, -0.00224, -0.0499, -0.0365, -0.0705, -0.0526, -0.0378, 0.00897, 0.0644, 0.0858, 0.131, 0.142, 0.121, 0.0766, 0.0229, -0.00897, -0.0231, 0.00755, 0.0185, 0.0555, 0.0407, 0.00459, -0.00836, -0.0189, -0.00204, 0.000612, -0.0119, -0.0214, -0.0309, -0.0522, -0.0741, -0.0590, -0.0271, -0.0182, 0.00408, 0.00999, 0.0140, 0.0225, 0.0446, 0.00928, -0.0559, -0.0566, -0.0248, -0.00826, 0.0255, 0.0418, 0.0186, -0.00275, -0.0248, -0.00153, 0.0252, 0.0491, 0.0798, 0.0634, 0.0338, -0.00143, -0.0199, -0.0252, -0.0216, -0.0112, 0.00510, 0.0246, -0.00347, -0.0220, -0.0480, -0.0370, -0.0199, -0.00184, 0.0173, -0.00816, 0.000510, 0.0235, 0.0381, 0.0613, 0.0526, 0.0441, 0.0351, 0.0515, 0.0666, 0.0696, 0.0175, -0.0173, -0.0537, -0.0677, -0.0395, -0.0226, -0.00337, 0.0121, -0.0131, -0.0358, -0.0524, -0.0342, -0.0222, -0.00122, 0.0145, 0.00714, -0.00642, -0.0122, -0.0328, -0.0353, -0.00928, 0.00744, 0.0315, 0.0481, 0.0615, 0.0587, 0.0337, -0.00744, -0.0792, -0.0620, -0.0447, -0.0213, 0.00316, 0.0357, 0.0299, 0.0123, 0.0345, 0.0323, 0.0259, 0.0210, 0.0202, 0.0177, 0.00214, -0.0147, -0.0350, -0.0346, -0.0148, -0.00286, 0.0173, -0.00979, -0.0260, -0.0284, -0.0396, -0.0247, -0.0219, -0.0186, -0.0177, -0.00387, -0.00275, -0.0189, -0.0125, 0.00887, 0.0350, 0.0709, 0.0928, 0.0870, 0.0775, 0.0523, 0.0190, 0.00153, -0.0194, -0.0154, -0.00744, 0.00214, 0.0132, 0.0219, 0.00245, -0.0126, -0.0335, -0.0529, -0.0722, -0.0590, -0.0471, -0.0313, -0.0148, -0.000918, -0.0184, -0.0324, -0.0474, -0.0399, -0.0352, -0.0322, -0.0444, -0.0501, -0.0484, -0.0428, -0.0368, -0.0282, -0.0263, -0.0142, -0.00693, 0.0517, 0.0736, 0.0895, 0.0797, 0.0780, 0.0448, 0.00816, 0.00133, -0.0128, -0.00153, 0.00306, 0.0106, 0.0106, 0.0197, 0.0209, 0.00755, -0.00571, -0.00734, 0.00714, 0.0108, 0.0150, -0.000918, -0.0162, -0.0191, -0.000714, 0.0158, 0.0107, -0.0117, -0.0308, -0.0315, -0.00969, -0.00591, 0.000408, 0.00204, 0.00510, 0.00581, 0.00989, 0.0137, 0.0180, 0.0222, 0.0266, 0.0308, 0.0353, 0.0394, 0.0483, 0.0401, 0.0243, 0.0117, -0.00806, -0.0126, 0.00551, 0.00275, -0.0255, -0.0577, -0.0642, -0.0603, -0.0421, -0.00693, 0.0277, 0.0282, -0.00214, -0.00612, -0.0112, -0.0225, -0.0424, -0.0529, -0.0226, 0.00306, 0.00806, 0.0142, 0.0174, 0.0258, 0.0329, 0.0399, 0.0167, -0.0139, -0.0329, -0.0297, -0.0293, -0.0310, -0.0346, -0.0250, -0.00775, 0.0127, 0.0383, 0.0410, 0.0250, 0.0159, -0.00408, -0.0156, -0.0295, -0.0322, -0.0113, 0.00959, 0.0342, 0.0587, 0.0432, 0.0146, -0.000714, -0.0138, -0.0275, -0.0348, -0.0364, -0.0404, -0.0410, -0.0498, -0.0489, -0.0414, -0.0415, -0.0358, -0.0191, -0.00581, 0.00449, -0.00194, -0.00734, -0.0172, -0.0117, 0.0128, 0.0365, 0.0667, 0.0730, 0.0777, 0.0754, 0.0640, 0.0494, 0.0269, -0.00449, -0.0294, -0.0392, -0.0502, -0.0436, -0.0424, -0.0281, -0.00530, 0.0242, 0.0434, 0.0616, 0.0461, 0.0290, 0.0128, -0.00551, -0.0280, -0.0431, -0.0178, 0.000102, 0.0259, 0.0490, 0.0654, 0.0565, 0.0421, 0.0188, -0.00489, -0.0309, -0.0541, -0.0722, -0.0946, -0.0880, -0.0643, -0.0383, 0.00887, 0.0315, 0.0601, 0.0626, 0.0393, 0.0358, 0.0317, 0.0251, 0.00194, -0.0202, -0.0161, -0.00153, 0.0101, 0.0292, 0.0416, 0.0574, 0.0541, 0.0320, 0.0168, -0.00245, -0.0193, -0.0281, -0.0378, -0.0459, -0.0545, -0.0493, -0.0386, -0.0302, -0.0200, -0.0188, -0.0162, -0.0121, -0.00540, 0.000204, 0.00602, -0.00235, -0.0114, -0.0209, -0.0328, -0.0396, -0.0348, -0.0293, -0.0334, -0.0415, -0.0497, -0.0574, -0.0657, -0.0566, -0.0455, -0.000816, 0.0258, 0.0419, 0.0657, 0.0590, 0.0483, 0.0392, 0.0393, 0.0347, 0.0364, 0.000816, -0.0259, -0.0469, -0.0480, -0.0226, -0.00663, 0.0167, 0.0362, 0.0514, 0.0378, 0.0286, 0.0161, 0.00418, -0.00275, 0.00214, 0.00449, 0.0101, 0.0138, 0.00959, 0.00591, 0.00224, -0.00316, -0.00316, 0.000408, -0.00255, -0.0126, -0.0240, -0.0414, -0.0540, -0.0715, -0.0329, -0.00530, 0.00959, 0.0334, 0.0487, 0.0519, 0.0365, 0.0349, 0.0312, 0.0291, 0.0268, 0.0171, -0.000510, -0.0216, -0.0372, -0.0316, -0.0303, -0.0286, -0.0242, -0.0271, -0.0314, -0.0373, -0.0364, -0.0314, -0.0197, 0.00194, 0.0200, 0.0163, 0.0132, 0.0143, 0.0112, 0.0110, 0.00938, 0.00908, 0.00194, -0.0134, -0.0252, -0.0445, -0.0441, -0.0306, -0.0196, -0.00489, 0.00989, 0.0171, 0.0151, 0.0176, 0.00795, -0.00591, -0.0219, -0.0239, 0.00612, 0.0267, 0.0274, 0.00857, -0.00418, -0.0231, -0.00775, 0.00316, 0.0186, 0.0316, 0.0488, 0.0468, 0.0169, -0.00591, -0.0404, -0.0453, -0.0246, -0.0103, 0.0221, 0.0266, 0.0123, 0.000816, -0.0171, -0.0390, -0.0577, -0.0796, -0.0631, -0.0261, 0.00449, 0.0465, 0.0798, 0.112, 0.0972, 0.0499, 0.0124, -0.0395, -0.0863, -0.125, -0.0881, -0.0628, -0.0232, 0.0153, 0.0486, 0.0329, 0.0253, 0.00969, 0.0231, 0.0345, 0.0515, 0.0606, 0.0529, 0.0563, 0.0607, 0.0629, 0.0605, 0.0475, 0.0161, -0.00785, -0.0235, -0.0309, -0.0335, -0.0371, -0.0500, -0.0673, -0.0737, -0.0799, -0.0863, -0.0577, -0.0213, 0.0131, 0.0389, 0.0498, 0.0370, 0.0255, 0.0161, 0.0384, 0.0606, 0.0642, 0.0377, 0.0221, 0.00408, 0.00235, -0.00632, -0.0292, -0.0387, -0.0466, -0.0515, -0.0792, -0.0640, -0.0193, 0.0193, 0.0519, 0.0759, 0.0764, 0.0573, 0.0464, 0.0535, 0.0648, 0.0745, 0.0868, 0.0945, 0.0994, 0.0947, 0.0830, 0.0269, -0.0295, -0.0919, -0.139, -0.0876, -0.0477, 0.00826, 0.0606, 0.0839, 0.0567, 0.0307, -0.00112, -0.0387, -0.0354, -0.00204, 0.0184, 0.0645, 0.103, 0.114, 0.113, 0.0959, 0.0821, 0.0620, 0.0518, 0.0210, -0.0205, -0.0583, -0.104, -0.136, -0.125, -0.123, -0.103, -0.0777, -0.0559, -0.0499, -0.0413, -0.0297, -0.0179, -0.0144, -0.00999, -0.00510, 0.00194, 0.00816, 0.00357, -0.00296, -0.00622, -0.00347, -0.00133, 0.00775, 0.0209, 0.0333, 0.0459, 0.0588, 0.0573, 0.0473, 0.0306, 0.0109, -0.000408, -0.00337, -0.00704, -0.0103, -0.0205, -0.0192, 0.00255, 0.0137, 0.0248, 0.0271, 0.0270, 0.0158, -0.00194, -0.00969, -0.0220, -0.0170, -0.0122, -0.00653, -0.0132, -0.0166, -0.0197, -0.0247, -0.0241, -0.0178, -0.0126, -0.0189, -0.0270, -0.0329, -0.0343, -0.0463, -0.0438, -0.0341, -0.0217, -0.00704, 0.00306, 0.000306, -0.00948, -0.00908, -0.0153, -0.0167, -0.0243, -0.0329, -0.0429, -0.0466, -0.0405, -0.0356, -0.0263, -0.0175, -0.00204, 0.0159, 0.0290, 0.0369, 0.0361, 0.0274, 0.0103, -0.00459, -0.0127, -0.0250, -0.0234, -0.0128, -0.00693, 0.00184, 0.00948, 0.0204, 0.0292, 0.0372, 0.0317, 0.0185, 0.00245, -0.0159, -0.0325, -0.0223, -0.0120, 0.00153, 0.0156, 0.0304, 0.0248, 0.0139, 0.0104, 0.00245, -0.00102, -0.00235, -0.00367, -0.00489, -0.00908, -0.0158, -0.0151, -0.00775, 0.0, -0.00204, -0.0151, -0.0229, -0.0381, -0.0372, -0.0256, -0.0167, -0.00143, 0.0156, 0.0308, 0.0401, 0.0414, 0.0393, 0.0335, 0.0187, 0.0127, 0.00714, -0.000102, -0.00693, -0.0133, -0.0131, -0.0117, -0.0104, -0.00857, -0.0144, -0.0209, -0.0274, -0.0357, -0.0366, -0.0303, -0.0231, 0.00184, 0.0101, 0.0207, 0.0261, 0.0203, 0.0163, 0.0109, 0.0159, 0.0202, 0.0247, 0.0144, 0.00561, -0.00673, -0.00989, -0.00265, 0.00163, 0.0102, 0.00959, 0.00398, 0.0, -0.00673, -0.0101, -0.0111, -0.0117, -0.0134, -0.0171, -0.0203, -0.0106, -0.00102, 0.00561, 0.0132, 0.0204, 0.0203, 0.0172, 0.0146, 0.0111, 0.0131, 0.0155, 0.0140, 0.0119, 0.00989, 0.00612, -0.00286, -0.00510, -0.00857, -0.0182, -0.0327, -0.0428, -0.0528, -0.0481, -0.0402, -0.0296, -0.0112, 0.00510, 0.0135, 0.0160, 0.0180, 0.0199, 0.0260, 0.0338, 0.0334, 0.0257, 0.0176, 0.00459, -0.00867, -0.0194, -0.0234, -0.0310, -0.0282, -0.0231, -0.0177, -0.0123, -0.0128, -0.0132, -0.00816, -0.00265, 0.00398, 0.00663, 0.00561, 0.00510, 0.00653, 0.0128, 0.0183, 0.0248, 0.0313, 0.0304, 0.0256, 0.0220, 0.0166, 0.0196, 0.0239, 0.0288, 0.0307, 0.0200, 0.0108, -0.00398, -0.0166, -0.0328, -0.0342, -0.0223, -0.0151, -0.00102, 0.00112, -0.00540, -0.0103, -0.0128, -0.0146, -0.0132, -0.0106, -0.00724, -0.00184, 0.00337, 0.00867, 0.0161, 0.0244, 0.0325, 0.0348, 0.0324, 0.0217, 0.00744, 0.000918, -0.00153, -0.00479, -0.00765, -0.0123, -0.0159, -0.0118, -0.00571, -0.000612, -0.000306, -0.000306, 0.00143, 0.00469, 0.00734, 0.00877, 0.00999, 0.0126, 0.0150, 0.0175, 0.0204, 0.0261, 0.0323, 0.0293, 0.0236, 0.0107, -0.000102, -0.00112, -0.00367, -0.00540, -0.00846, -0.00530, -0.000714, 0.00377, 0.00979, 0.0158, 0.0209, 0.0146, 0.00744, 0.00214, -0.00224, -0.00714, -0.0102, -0.00765, -0.00551, -0.00296, -0.00153, 0.0, -0.000102, -0.000612, -0.00112, -0.00102, -0.000306, 0.000102, 0.00163, 0.00540, 0.00877, 0.0128, 0.0157, 0.0134, 0.0104, 0.00571, 0.000612, -0.00408, -0.00999, -0.00979, -0.00469, -0.000714, 0.00316, 0.00459, 0.00693, 0.00530, 0.00459, 0.00184, -0.000204, -0.00296, -0.00286, -0.00173, -0.000612, -0.000510, -0.00173, -0.00224, -0.00367, -0.00326, -0.000714, 0.00143, 0.00418, 0.00663, 0.00642, 0.00530, 0.000612, -0.00459, -0.00775, -0.00653, -0.00663, -0.0109, -0.0164, -0.0177, -0.0119, -0.00714, -0.00367, -0.00255, -0.000204, 0.000408, 0.00367, 0.00836, 0.0117, 0.00714, 0.00387, -0.00102, -0.00255, -0.00347, -0.00418, -0.00459, -0.00663, -0.0122, -0.0119, -0.0110, -0.00999, -0.00826, -0.00775, -0.0112, -0.0141, -0.0169, -0.0147, -0.0133, -0.0113, -0.0110, -0.00999, -0.00999, -0.00642, -0.000204, 0.00551, 0.0112, 0.0155, 0.0201, 0.0173, 0.0143, 0.0105, 0.00653, 0.00102, -0.00683, -0.0107, -0.0124, -0.0141, -0.0169, -0.0219, -0.0274, -0.0279, -0.0221, -0.0182, -0.0118, -0.00622, 0.0, 0.00551, 0.0101, 0.0147, 0.0191, 0.0237, 0.0267, 0.0260, 0.0231, 0.0174, 0.0130, 0.00551, -0.00479, -0.0152, -0.0214, -0.0259, -0.0257, -0.0250, -0.0230, -0.0183, -0.0139, -0.00836, -0.00143, 0.00449, 0.00908, 0.00989, 0.0102, 0.0102, 0.0106, 0.0139, 0.0123, 0.0102, 0.00734, 0.00194, -0.00286, -0.00846, -0.0134, -0.0194, -0.0198, -0.0142, -0.0102, -0.00449, -0.00449, -0.00408, -0.00459, -0.00500, -0.00398, -0.00214, -0.00224, -0.00163, -0.00163, -0.00122, -0.00122, 0.00286, 0.00693, 0.0117, 0.00979, 0.00571, 0.00173, -0.00316, -0.00795, -0.00938, -0.00938, -0.00969, -0.00938, -0.00877, -0.00530, -0.00224, 0.00143, 0.00387, 0.00347, 0.00347, 0.00296, 0.00408, 0.00551, 0.00683, 0.00867, 0.00887, 0.00785, 0.00714, 0.00622, 0.00765, 0.00887, 0.0104, 0.0113, 0.00918, 0.00765, 0.00540, 0.00632, 0.00806, 0.00989, 0.0120, 0.0138, 0.0107, 0.00785, 0.00449, 0.00122, -0.00214, -0.00428, -0.00693, -0.00887, -0.0122, -0.0162, -0.0202, -0.0238, -0.0263, -0.0275, -0.0283, -0.0293, -0.0283, -0.0275, -0.0263, -0.0256, -0.0223, -0.0176, -0.0132, -0.0111, -0.0107, -0.00969, -0.00928, -0.00734, -0.00602, -0.00408, -0.00235, 0.00184, 0.00357, 0.00428, 0.00489, 0.00438, 0.00438, 0.00387, 0.00367, 0.00326, 0.00479, 0.00642, 0.00867, 0.0112, 0.0137, 0.0163, 0.0185, 0.0176, 0.0171, 0.0162, 0.0150, 0.0110, 0.00693, 0.00326, 0.000918, 0.00194, 0.00286, 0.00408, 0.00551, 0.00704, 0.00969, 0.0119, 0.0146, 0.0146, 0.0127, 0.0124, 0.0126, 0.0128, 0.0133, 0.0136, 0.0108, 0.00806, 0.00469, 0.00204, 0.000510, -0.00112, -0.00255, -0.00112, 0.00184, 0.00438, 0.00775, 0.0105, 0.0136, 0.0103, 0.00632, 0.00184, -0.00275, -0.00816, -0.00908, -0.00551, -0.00306, 0.000612, 0.00122, 0.00224, 0.00275, 0.00326, 0.00489, 0.00704, 0.00908, 0.0114, 0.0136, 0.0143, 0.0140, 0.0143, 0.0138, 0.0142, 0.0104, 0.00306, -0.00326, -0.00642, -0.00867, -0.0114, -0.0160, -0.0201, -0.0192, -0.0186, -0.0173, -0.0159, -0.0145, -0.0132, -0.0127, -0.0122, -0.0118, -0.0116, -0.0108, -0.00653, -0.00214, 0.000204, -0.000408, -0.000408, -0.00143, -0.00194, -0.000816, 0.00112, 0.00275, 0.00510, 0.00540, 0.00428, 0.00306, 0.00112, -0.000510, -0.00245, -0.00204, -0.000714, 0.000408, 0.00194, 0.00337, 0.00489, 0.00581, 0.00663, 0.00755, 0.00826, 0.00928, 0.0130, 0.0172, 0.0147, 0.0107, 0.00714, 0.00357, 0.000204, -0.00326, -0.00653, -0.00969, -0.00928, -0.00846, -0.00744, -0.00520, -0.00306, -0.000408, 0.00337, 0.00530, 0.00449, 0.00408, 0.00306, 0.00204, 0.00102, 0.000102, -0.00102, -0.00143, 0.000204, 0.00163, 0.00306, 0.00235, 0.00163, 0.00163, 0.00265, 0.00326, 0.00459, 0.00469, 0.00214, -0.000306, -0.00306, -0.00612, -0.00887, -0.0118, -0.0109, -0.00959, -0.00795, -0.00693, -0.00642, -0.00581, -0.00510, -0.00418, -0.00337, -0.00245, -0.00143, -0.000510, 0.000408, 0.00133, 0.00214, 0.000102, -0.00184, -0.00408, -0.00653, -0.00887, -0.0112, -0.0132, -0.0154, -0.0168, -0.0177, -0.0188, -0.0198, -0.0206, -0.0217, -0.0202, -0.0176, -0.0138, -0.00663, -0.00143, 0.00204, 0.00561, 0.00836, 0.00908, 0.0104, 0.00948, 0.00816, 0.00622, 0.00337, 0.000306, -0.00245, -0.00581, -0.00459, -0.00133, 0.00133, 0.00337, 0.00489, 0.00551, 0.00591, 0.00632, 0.00663, 0.00704, 0.00734, 0.00765, 0.00785, 0.00744, 0.00704, 0.00653, 0.00591, 0.00551, 0.00489, 0.00622, 0.00806, 0.00979, 0.0116, 0.0128, 0.0142, 0.0134, 0.0125, 0.0116, 0.0105, 0.00887, 0.00734, 0.00571, 0.00408, 0.00245, 0.000714, 0.000408, 0.000816, 0.000918, 0.00235, 0.00428, 0.00530, 0.00622, 0.00734, 0.00806, 0.00908, 0.00969, 0.00969, 0.00948, 0.00908, 0.00255, 0.0, -0.000510, -0.00204, -0.00245, -0.00337, -0.00489, -0.00653, -0.00816, -0.00989, -0.0116, -0.0131, -0.0108, -0.00908, -0.00673, -0.00438, -0.00194, -0.00112, -0.000918, -0.000510, -0.000408, 0.000510, 0.00143, 0.00245, 0.00357, 0.00438, 0.00449, 0.00398, 0.00133, -0.000816, -0.00367, -0.00479, -0.00561, -0.00632, -0.00693, -0.00744, -0.00816, -0.00765, -0.00653, -0.00286, 0.000510, 0.00469, 0.00663, 0.00612, 0.00612, 0.00551, 0.00500, 0.00449, 0.00398, 0.00296, 0.0, -0.00265, -0.00530, -0.00428, -0.00377, -0.00255, -0.00153, -0.000204, 0.000204, 0.0, -0.000102, -0.000408, -0.000714, -0.000612, -0.000102, 0.000306, 0.000714, -0.000102, -0.000714, -0.00153, -0.00245, -0.00296, -0.00286, -0.00286, -0.00255, -0.00245, -0.00214, -0.00275, -0.00449, -0.00591, -0.00775, -0.00857, -0.00836, -0.00846, -0.00765, -0.00510, -0.00286, -0.000204, 0.00184, 0.00184, 0.00214, 0.00214, 0.00214, 0.00316, 0.00438, 0.00500, 0.00500, 0.00500, 0.00489, 0.00479, 0.00530, 0.00683, 0.00806, 0.00969, 0.00989, 0.00979, 0.00979, 0.00948, 0.00928, 0.00908, 0.00887, 0.00857, 0.00836, 0.00816, 0.00785, 0.00632, 0.00479, 0.00357, 0.00367, 0.00347, 0.00357, 0.00367, 0.00326, 0.00194, 0.00153, 0.00143, 0.00122, 0.00122, 0.00112, 0.0, -0.00102, -0.00204, -0.00255, -0.00296, -0.00337, -0.00377, -0.00489, -0.00642, -0.00785, -0.00938, -0.0109, -0.0124, -0.0141, -0.0146, -0.0122, -0.0105, -0.00795, -0.00683, -0.00642, -0.00591, -0.00571, -0.00540, -0.00510, -0.00489, -0.00459, -0.00428, -0.00398, -0.00316, -0.00265, -0.00112, 0.00102, 0.00316, 0.00469, 0.00489, 0.00551, 0.00561, 0.00642, 0.00857, 0.0103, 0.00857, 0.00724, 0.00540, 0.00337, 0.00286, 0.00296, 0.00286, 0.00275, 0.00122, -0.000102, -0.00102, -0.00112, -0.00163, -0.00153, -0.00316, -0.00693, -0.0101, -0.0104, -0.0107, -0.0108, -0.0101, -0.00908, -0.00806, -0.00755, -0.00734, -0.00693, -0.00683, -0.00642, -0.00408, -0.00133, 0.00133, 0.00438, 0.00724, 0.0101, 0.00877, 0.00642, 0.00418, 0.00133, -0.00143, -0.00357, -0.00377, -0.00449, -0.00469, -0.00500, -0.00520, -0.00551, -0.00571, -0.00591, -0.00469, -0.00296, -0.00133, 0.000714, 0.00173, 0.00194, 0.00255, 0.00255, 0.00357, 0.00510, 0.00653, 0.00816, 0.00959, 0.00989, 0.0104, 0.0107, 0.0110, 0.0112, 0.0106, 0.0101, 0.00948, 0.00877, 0.00806, 0.00734, 0.00642, 0.00561, 0.00469, 0.00387, 0.00316, 0.00275, 0.00214, 0.00173, 0.00122, 0.000714, 0.000306, 0.000816, 0.00133, 0.00194, 0.00224, 0.000918, -0.000306, -0.00173, -0.00316, -0.00479, -0.00540, -0.00489, -0.00438, -0.00387, -0.00255, -0.000918, 0.000714, 0.00173, 0.00224, 0.00357, 0.00622, 0.00846, 0.0114, 0.0109, 0.00887, 0.00724, 0.00479, 0.00275, 0.000408, -0.00163, -0.00245, -0.00337, -0.00469, -0.00704, -0.00857, -0.00897, -0.00948, -0.00969, -0.00989, -0.0101, -0.0103, -0.0104, -0.00999, -0.00969, -0.00918, -0.00887, -0.00908, -0.00928, -0.00948, -0.00979, -0.00989, -0.00897, -0.00826, -0.00724, -0.00693, -0.00693, -0.00683, -0.00683, -0.00673, -0.00673, -0.00653, -0.00571, -0.00510, -0.00438, -0.00347, -0.00235, -0.00133, -0.000204, 0.00102, 0.00173, 0.000918, 0.000306, -0.000510, -0.00143, -0.000714, 0.000612, 0.00163, 0.00133, 0.00122, 0.000816, 0.000510, 0.0, 0.000204, 0.000918, 0.00143, 0.00214, 0.00224, 0.00102, 0.0, -0.00122, -0.00245, -0.00245, -0.00194, -0.00163, -0.00184, -0.00224, -0.00275, -0.00306, -0.00265, -0.00224, -0.00224, -0.00326, -0.00418, -0.00520, -0.00469, -0.00438, -0.00387, -0.00326, -0.00265, -0.00204, -0.00194, -0.00184, -0.00153, 0.00133, 0.00418, 0.00683, 0.00714, 0.00775, 0.00795, 0.00816, 0.00826, 0.00857, 0.00795, 0.00622, 0.00489, 0.00316, 0.00296, 0.00306, 0.00326, 0.00357, 0.00377, 0.00245, 0.000714, -0.000816, -0.00286, -0.00357, -0.00224, -0.00133, 0.000102, 0.000306, 0.000102, -0.000102, -0.000510, -0.000816, -0.00112, -0.00153, -0.00122, -0.000816, -0.000408, 0.000102, 0.000510, 0.000816, 0.000816, 0.000918, 0.00102, 0.00224, 0.00337, 0.00469, 0.00602, 0.00734, 0.00806, 0.00632, 0.00510, 0.00326, 0.00163, -0.000408, 0.0, 0.00306, 0.00530, 0.00887, 0.00836, 0.00316, -0.000204, -0.00632, -0.00286, 0.00591, 0.0133, 0.0225, 0.0189, 0.00959, 0.00928, 0.00785, 0.00918, 0.00163, -0.00857, -0.0204, -0.0151, -0.000714, 0.0109, 0.0101, 0.00489, 0.000306, -0.00704, -0.0109, -0.00153, 0.00704, 0.0141, 0.00683, 0.00204, 0.00357, 0.00918, 0.00877, 0.00530, 0.00245, 0.00163, 0.00133, -0.00112, -0.00347, -0.00683, -0.00551, -0.00204, 0.00143, 0.00489, 0.00989, 0.00591, -0.00489, -0.0121, -0.00969, -0.00612, -0.00245, 0.00326, 0.00387, 0.00143, -0.000102, -0.000714, 0.000612, 0.00102, 0.00245, 0.00133, 0.000408, -0.000816, -0.00275, -0.00561, -0.00857, -0.00948, -0.00622, -0.00367, -0.000204, 0.00337, 0.00377, -0.00143, -0.00459, -0.0105, -0.00785, -0.00235, 0.00316, 0.00418, 0.00265, 0.00133, 0.00153, 0.00622, 0.00744, 0.00755, 0.00775, 0.00744, 0.00724, 0.00581, 0.00387, 0.00214, -0.000204, -0.00337, -0.00653, -0.00693, -0.00520, -0.00551, -0.00551, -0.00581, -0.00337, 0.0, 0.00337, 0.00714, 0.00979, 0.00867, 0.00816, 0.00693, 0.00765, 0.00785, 0.00867, 0.00622, 0.00377, 0.000714, -0.000102, 0.000408, 0.000510, -0.00102, -0.00296, -0.00469, -0.00704, -0.00663, -0.00449, -0.00286, -0.000714, -0.00235, -0.00337, -0.00520, -0.00612, -0.00571, -0.00551, -0.00510, -0.00673, -0.00928, -0.0116, -0.0143, -0.0141, -0.0143, -0.0123, -0.00979, -0.00755, -0.00469, -0.00347, -0.00428, -0.00459, -0.00561, -0.00642, -0.00734, -0.00816, -0.00744, -0.00530, -0.00377, -0.00265, -0.00143
])
DT = 0.02


################################################################################
# BUILDING PARAMETERS — SAME AS CODE 2
################################################################################

mass = 202472.0
height = 14.0
stiffness = 47.52e6
zeta = 0.05
Fy = 500000.0
b = 0.02

omega = np.sqrt(stiffness / mass)
period = 2*np.pi / omega
c = 2*zeta*np.sqrt(stiffness*mass)
uy = Fy / stiffness

print("\nBuilding properties:")
print(f"  Mass      = {mass:.1f} kg")
print(f"  Stiffness = {stiffness/1e6:.2f} MN/m")
print(f"  Damping   = {zeta*100:.1f}%")
print(f"  Period    = {period:.3f}s")
print(f"  Yield disp= {uy*1000:.2f} mm")


################################################################################
# DAMAGE STATES — EXACT IS 1893
################################################################################

damage_states = {
    "DS1_Slight":    0.004 * height,
    "DS2_Moderate":  0.008 * height,
    "DS3_Extensive": 0.015 * height,
    "DS4_Complete":  0.025 * height
}


################################################################################
# OPENSEES SOLVER — Steel01 / Newmark
################################################################################

def run_opensees(pga_target):

    ops.wipe()

    ops.model('basic', '-ndm', 1, '-ndf', 1)

    ops.node(1, 0.0)
    ops.node(2, 0.0)

    ops.fix(1, 1)
    ops.mass(2, mass)

    dt = DT
    scale_factor = pga_target / np.max(np.abs(AG))
    acc = AG * scale_factor * 9.81

    ops.uniaxialMaterial("Steel01", 1, Fy, stiffness, b)
    ops.element("zeroLength", 1, 1, 2, "-mat", 1, "-dir", 1)

    alphaM = 2*zeta*omega
    ops.rayleigh(alphaM, 0, 0, 0)

    ops.timeSeries("Path", 1, "-dt", dt, "-values", *acc.tolist())
    ops.pattern("UniformExcitation", 1, 1, "-accel", 1)

    ops.recorder("Node", "-file", "temp_disp.out", "-time", "-node", 2, "-dof", 1, "disp")

    ops.constraints("Plain")
    ops.numberer("RCM")
    ops.system("BandGeneral")
    ops.algorithm("Newton")
    ops.integrator("Newmark", 0.5, 0.25)
    ops.analysis("Transient")

    for _ in range(len(acc)):
        ops.analyze(1, dt)

    disp = np.loadtxt("temp_disp.out")[:,1]
    return np.max(np.abs(disp))


################################################################################
# TRAINING SET GENERATION
################################################################################

def generate_training(N=40):
    print("\nGenerating training data...")
    pgas = np.sort(np.random.uniform(0.05, 3.0, N))

    data = []
    for i, p in enumerate(pgas):
        u = run_opensees(p)
        drift = u / height
        data.append({"pga": p, "u_max": u, "drift": drift})
        print(f"  {i+1}/{N}: PGA={p:.2f}, u_max={u*1000:.2f} mm")

    return data


################################################################################
# SURROGATE NN — SAME ARCHITECTURE AS CODE 2
################################################################################

class Surrogate(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(1, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Softplus()
        )

    def forward(self, x):
        return self.net(x)


def train_surrogate(data, epochs=8000):

    model = Surrogate().to(device)
    opt = optim.Adam(model.parameters(), lr=1e-3)

    X = torch.tensor([[d['pga']] for d in data], dtype=torch.float32, device=device)
    y = torch.tensor([[d['u_max']] for d in data], dtype=torch.float32, device=device)

    losses = []
    r2_values = []

    print("\nTraining surrogate...")

    for ep in range(epochs):
        opt.zero_grad()

        y_pred = model(X)
        loss = torch.mean((y_pred - y)**2)

        loss.backward()
        opt.step()

        losses.append(loss.item())

        # R2 each epoch
        ss_res = torch.sum((y - y_pred)**2)
        ss_tot = torch.sum((y - torch.mean(y))**2)
        r2 = 1 - ss_res/ss_tot
        r2_values.append(r2.item())

        if (ep+1) % 2000 == 0:
            print(f"  Epoch {ep+1}: Loss={loss.item():.5e}, R2={r2.item():.4f}")

    return model, losses, r2_values


################################################################################
# PERFORMANCE METRICS
################################################################################

def evaluate_model(model, data):

    X = torch.tensor([[d['pga']] for d in data], dtype=torch.float32, device=device)
    y_true = np.array([d['u_max'] for d in data])
    y_pred = model(X).detach().cpu().numpy().flatten()

    errors = y_pred - y_true

    metrics = {
        "R2": 1 - np.sum((y_true-y_pred)**2)/np.sum((y_true-np.mean(y_true))**2),
        "MSE": np.mean(errors**2),
        "RMSE": np.sqrt(np.mean(errors**2)),
        "MAE": np.mean(np.abs(errors)),
        "MAPE": np.mean(np.abs(errors)/y_true)*100,
        "Max Error": np.max(errors),
        "Min Error": np.min(errors),
        "Bias": np.mean(errors),
        "Std Error": np.std(errors),
        "Scatter Index": np.std(errors)/np.mean(y_true),
        "NRMSE": np.sqrt(np.mean(errors**2))/np.max(y_true)
    }

    print("\nPERFORMANCE METRICS")
    for k,v in metrics.items():
        print(f"  {k:15s} = {v:.6f}")

    return y_pred, metrics


################################################################################
# IDA
################################################################################

def run_ida(model, pga_levels):
    ida = []
    for p in pga_levels:
        p_tensor = torch.tensor([[p]], dtype=torch.float32, device=device)
        with torch.no_grad():
            u = model(p_tensor).item()
        ida.append((p,u))
    return ida


################################################################################
# MONTE CARLO
################################################################################

def run_mc(model, pga_levels, n=400):

    results = []

    for p in pga_levels:
        pga_mc = np.random.normal(p, 0.1*p, n)
        pga_mc = np.clip(pga_mc, p*0.7, p*1.3)

        X = torch.tensor(pga_mc.reshape(-1,1), dtype=torch.float32, device=device)
        with torch.no_grad():
            preds = model(X).cpu().numpy().flatten()

        for u in preds:
            results.append({"pga": p, "u_max": u})

    df = pd.DataFrame(results)
    return df


################################################################################
# FRAGILITY (SMOOTH — 2000 POINTS — NO DOTS)
################################################################################

def compute_fragility(df_mc, pga_levels, damage_states):

    curves = {}

    for ds, limit in damage_states.items():

        probs = []
        for pga in pga_levels:
            dfp = df_mc[df_mc['pga']==pga]
            p = np.mean(dfp['u_max'] >= limit)
            probs.append(p)

        probs = np.array(probs)

        # Fit
        def nll(params):
            th, be = params
            if th<=0 or be<=0: return 1e12
            P = norm.cdf((np.log(pga_levels)-np.log(th))/be)
            P = np.clip(P,1e-10,1-1e-10)
            return -np.sum(probs*np.log(P)+(1-probs)*np.log(1-P))

        idx = np.argmin(np.abs(probs-0.5))
        th0 = pga_levels[idx] if probs[idx]>0 else 1.0

        res = minimize(nll, [th0,0.4], bounds=[(0.01,10),(0.1,1.5)])
        theta,beta = res.x

        im = np.linspace(0.01, max(pga_levels)*1.5, 2000)
        P = norm.cdf((np.log(im)-np.log(theta))/beta)

        curves[ds] = {"theta":theta, "beta":beta, "IM":im, "P":P}

    return curves


################################################################################
# PLOTS — FULL WINDOW, NO SUBPLOTS, MAGAZINE STYLE
################################################################################

def large_plot(title, xlabel, ylabel):
    plt.figure(figsize=(14,9))
    plt.title(title, fontsize=20)
    plt.xlabel(xlabel, fontsize=16)
    plt.ylabel(ylabel, fontsize=16)
    plt.grid(alpha=0.3)


def plot_training(losses, r2_values):

    # LOSS
    large_plot("Training Loss Curve", "Epoch", "Loss")
    plt.plot(losses, linewidth=2)
    plt.show()

    # R²
    large_plot("Training R² Curve", "Epoch", "R² Score")
    plt.plot(r2_values, linewidth=2)
    plt.ylim([0,1.05])
    plt.show()


def plot_surrogate_performance(data, y_pred):

    y_true = np.array([d['u_max'] for d in data])

    large_plot("Surrogate Model Performance", "True Displacement (mm)", "Predicted (mm)")

    plt.scatter(y_true*1000, y_pred*1000, s=40, alpha=0.6)
    lim = [0,max(max(y_true),max(y_pred))*1000]
    plt.plot(lim,lim,'r--')
    plt.show()


def plot_ida(ida):

    large_plot("IDA Curve", "PGA (g)", "Max Displacement (mm)")

    x = [p for p,u in ida]
    y = [u*1000 for p,u in ida]
    plt.plot(x,y,'b-',linewidth=3)
    plt.show()


def plot_fragility(curves):

    large_plot("Fragility Curves", "PGA (g)", "Probability of Exceedance")

    colors = ["blue", "gold", "red", "darkred"]

    for (ds,c),color in zip(curves.items(),colors):
        plt.plot(c["IM"], c["P"], linewidth=3, color=color, label=f"{ds} (θ={c['theta']:.2f}, β={c['beta']:.2f})")

    plt.legend(fontsize=14)
    plt.ylim([0,1])
    plt.show()


################################################################################
# FINAL EXECUTION
################################################################################

def main():

    data = generate_training(40)
    model, losses, r2_values = train_surrogate(data)

    y_pred, metrics = evaluate_model(model, data)

    plot_training(losses, r2_values)
    plot_surrogate_performance(data, y_pred)

    pga_levels = np.array([0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.8,1.0,1.2,1.5,2.0,2.5,3.0])
    ida = run_ida(model, pga_levels)
    plot_ida(ida)

    df_mc = run_mc(model, pga_levels, n=500)

    curves = compute_fragility(df_mc, pga_levels, damage_states)
    plot_fragility(curves)

    print("\n\n---------------------------------------------")
    print(" MODEL BEHAVIOR & WHAT WAS DONE")
    print("---------------------------------------------")

if __name__ == "__main__":
    main()


Results:

Building properties:
  Mass      = 202472.0 kg
  Stiffness = 47.52 MN/m
  Damping   = 5.0%
  Period    = 0.410s
  Yield disp = 10.52 mm

Generating training data...
  1/40: PGA=0.11, u_max=7.82 mm
  2/40: PGA=0.19, u_max=16.57 mm
  3/40: PGA=0.22, u_max=17.64 mm
  4/40: PGA=0.24, u_max=18.28 mm
  5/40: PGA=0.34, u_max=27.05 mm
  6/40: PGA=0.46, u_max=44.94 mm
  7/40: PGA=0.51, u_max=51.65 mm
  8/40: PGA=0.51, u_max=51.66 mm
  9/40: PGA=0.55, u_max=55.99 mm
  10/40: PGA=0.59, u_max=58.54 mm
  11/40: PGA=0.59, u_max=58.94 mm
  12/40: PGA=0.64, u_max=62.25 mm
  13/40: PGA=0.68, u_max=63.92 mm
  14/40: PGA=0.91, u_max=109.18 mm
  15/40: PGA=0.91, u_max=109.98 mm
  16/40: PGA=0.95, u_max=118.58 mm
  17/40: PGA=0.95, u_max=118.86 mm
  18/40: PGA=1.13, u_max=163.12 mm
  19/40: PGA=1.15, u_max=168.64 mm
  20/40: PGA=1.32, u_max=194.17 mm
  21/40: PGA=1.35, u_max=196.73 mm
  22/40: PGA=1.40, u_max=201.79 mm
  23/40: PGA=1.57, u_max=226.45 mm
  24/40: PGA=1.60, u_max=231.58 mm
  25/40: PGA=1.80, u_max=268.59 mm
  26/40: PGA=1.82, u_max=272.17 mm
  27/40: PGA=1.82, u_max=273.58 mm
  28/40: PGA=1.84, u_max=277.28 mm
  29/40: PGA=1.85, u_max=279.68 mm
  30/40: PGA=2.07, u_max=321.69 mm
  31/40: PGA=2.14, u_max=335.90 mm
  32/40: PGA=2.21, u_max=350.38 mm
  33/40: PGA=2.37, u_max=383.74 mm
  34/40: PGA=2.43, u_max=398.80 mm
  35/40: PGA=2.51, u_max=414.75 mm
  36/40: PGA=2.61, u_max=437.11 mm
  37/40: PGA=2.85, u_max=493.60 mm
  38/40: PGA=2.85, u_max=494.82 mm
  39/40: PGA=2.90, u_max=504.81 mm
  40/40: PGA=2.91, u_max=507.67 mm

Training surrogate...
  Epoch 2000: Loss=2.20875e-06, R2=0.9999
  Epoch 4000: Loss=3.80537e-06, R2=0.9998
  Epoch 6000: Loss=6.53992e-07, R2=1.0000
  Epoch 8000: Loss=2.69060e-07, R2=1.0000

PERFORMANCE METRICS
  R2              = 0.999989
  MSE             = 0.000000
  RMSE            = 0.000519
  MAE             = 0.000309
  MAPE            = 0.479516
  Max Error       = 0.001465
  Min Error       = -0.001591
  Bias            = -0.000001
  Std Error       = 0.000519
  Scatter Index   = 0.002473
  NRMSE           = 0.001022
